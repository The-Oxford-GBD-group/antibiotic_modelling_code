% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_xgboost_child_model.R
\name{fit_xgboost_child_model}
\alias{fit_xgboost_child_model}
\title{Fit xgboost child model function}
\usage{
fit_xgboost_child_model(df, indicator = indicator,
  indicator_family = "binomial", outputdir, region,
  covariates = all_fixed_effects, weight_column = "weight",
  xg_model_tune = TRUE, hyperparameter_filepath = NULL)
}
\arguments{
\item{df}{Data Frame. Data frame that contains your indicator and the covariate values extracted at those cluster locations}

\item{indicator}{Character. Model indicator. Make sure in your data frame that this exists}

\item{indicator_family}{Character. Indicator statistical family. Common inputs include Binomial and Gaussian}

\item{outputdir}{File path. Model output directory, defined early on in the parallel script}

\item{region}{Character. Region being modeled over}

\item{covariates}{Character. List of covariates being used in stacking. For example: access2 + aridity + fertility}

\item{weight_column}{Numeric. This column contains the weights form polygon resampling.}

\item{xg_model_tune}{Logical. If true, xgboost will perform grid search to pick best hyperparameters}

\item{hyperparameter_filepath}{File path. If true, xgboost will perform a grid search to find the optimal hyperparameter.
If false, it will read from config what the optimal parameters. You must specify nrounds, eta and max depth}
}
\value{
a list containing: 1) The out of sample and in sample predictions from the xgboost fit, labeled "dataset"
                           2) The child model object, labeled "xgboost"
}
\description{
This function fits xgboost, another implementation of boosted regression trees. Xgboost has a different fitting algorithm
as compared to gbm, and tends to fit much faster with fewer number of iterations required. This speeds up the run time considerably.

This function takes a data frame with the extracted covariates at each lat/long and fits regression trees on your indicator.
The added bonus of xgboost for prevalence indicators is it can perform logistic regression with an outcome variable between
\code{[0,1]}, so we no longer need to convert brt's to a poisson distribution.

This function also has built in cross-validation that uses 5 fold cross validation repeated 5 times for model accuracy.
You can either pass the hyperparameter space as a file path to a csv file, or go with the default grid search by not providing a filepath.
Xgboost has a large number of parameters, but as of now the only 3 tunable hyperparameters. The number of rounds (nround) specifies the number of iterations.
Eta (learning rate) which specifies the shrinkage used in updating each tree fit to prevent overfitting. After each boosting step, we can
directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative. Finally, we also vary
the max depth, which specifies the maximum depth of the tree. Increasing this value will make the model more complex and more likely to overfit.
}
\author{
Michael Cork, \email{mcork23\@uw.edu}
}
