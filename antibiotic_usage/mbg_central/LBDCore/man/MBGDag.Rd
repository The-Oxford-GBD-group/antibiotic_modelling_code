% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MBGDag.R
\docType{class}
\name{MBGDag}
\alias{MBGDag}
\title{MBG DAG Class for Parallel Model.}
\format{\code{\link{R6Class}} object.}
\usage{
MBGDag
}
\value{
Object of \code{\link{R6Class}} with the DAG for the parallel model.
}
\description{
MBG DAG Class for Parallel Model.
}
\section{Fields}{

\describe{
\item{\code{pipeline_class}}{An \code{MBGPipeline} object}

\item{\code{dag_hash}}{A 5 character random string used to hash the nodes.
The hash is randomly generated if not supplied by user.}

\item{\code{reg}}{Region}

\item{\code{age}}{Age}

\item{\code{holdout}}{Holdout fold}

\item{\code{test}}{Test mode?}

\item{\code{tmpdir}}{Path to temporary directory to start job markers in.
Default: \code{"~/.mbgdir"}.}
}}

\section{DAG Table}{

The DAG table will consist of the following information about the
nodes (the jobs):
\itemize{
\item{\code{nodename}}: Name of node in DAG

\item{\code{node_counter}: Auto-incrementing counter specifying the
node number in the graph
}

\item{\code{jobscript}: Path to the R script being run
}

\item{\code{job_name}: Name of job
}

\item{\code{project}: Cluster project
}

\item{\code{queue}: Queue
}

\item{\code{fthread}: Number of CPUs requested
}

\item{\code{m_mem_free}: Amount of RAM requested
}

\item{\code{h_rt}: Runtime requested
}

\item{\code{qsub_str}: The qsub string
}

\item{\code{status}: Job status
}

\item{\code{job_id}: Job ID (after job has been submitted)
}

}
}

\section{Methods}{

\describe{
\item{\code{print}}{
  Prints the DAG info
}


\item{\code{argparse_append}}{
 Prepare a parameter to be passed as argparse input into jobs. Return just the value if \code{use_argparse = FALSE}
  \emph{Arguments:}
  \itemize{
    \item{\code{val}: Argument value
    }
    \item{\code{val_lab}: Argparse field, parsed as: \code{"-- val_lab val"}
    }
    \item{\code{use_argparse}: Use argparse to create arguments? Default: \code{FALSE}.
     }
  }
}


\item{\code{clean_dag}}{
 Empties the DAG object
}

\item{\code{get_prerun_image_path}}{
 Evaluate a config parameter
  \emph{Arguments:}
  \itemize{
    \item{\code{ig}: Indicator group, from the pipeline object.
    }
    \item{\code{indic}: Indicator, from the pipeline object.
    }
    \item{\code{rd}: Run date, from the pipeline object.
    }
    \item{\code{file_format}: File format suffix. Default: \code{".rds"}
    }
  }
}

\item{\code{create_node}}{
 Create a node (job) for the DAG
  \emph{Arguments:}
  \itemize{
    \item{\code{nodename}: Name of node.
    }
    \item{\code{jobscript}: Path to R script.
    }
    \item{\code{project}: Cluster project. Default: \code{"proj_geo_nodes"}.
    }
    \item{\code{cores}: Number of threads. Default: 1.
    }
    \item{\code{ram_gb}: Amount of RAM. Default: 5 (5 GB).
    }
    \item{\code{runtime}: Run-time. Default: \code{"01:00:00"} (1 hour).
    }
    \item{\code{queue}: Queue. Default: \code{"geospatial.q"}.
    }
    \item{\code{hold_job}: Node to hold on. Default: \code{"previous"},
     which will create a hold on the previous job. Alternatively, the node
     on which to hold can also be specified.
    }
    \item{\code{log_location}: Location to store logs to feed into
     \code{setup_log_location}. Default: \code{"sharedir"}
    }
    \item{\code{singularity_version}: Singularity image (version or absolute
    path to image). Default: \code{"default"}
    }
    \item{\code{singularity_opts}: A list with values for
    \code{SET_OMP_THREADS} and  \code{SET_MKL_THREADS} to pass in OMP and
    MKL threads. Defaults to using the same amount of threads for each as
    the number of cores.
    }
    \item{\code{shell_script}: Path to shell script. Note that using
    argparse requires a special shell script (\code{shell_sing_argparse.sh}).
    }
  }
}

\item{\code{get_dag_value}}{
 Get field from the DAG given the nodename
  \emph{Arguments:}
  \itemize{
    \item{\code{name}: Node name.
    }
    \item{\code{counter}: Node counter.
    }
    \item{\code{field}: Field to get value from.
    }
  }
}

\item{\code{get_all_parent_nodes}}{
 Get all the parent nodes for a given node name. This is helpful if you want
to hold on a whole set of jobs
  \emph{Arguments:}
  \itemize{
    \item{\code{name}: Node name
    }
  }
}

\item{\code{update_dag}}{
 Update a value of a field in the DAG table for a given node.
  \emph{Arguments:}
  \itemize{
    \item{\code{name}: Node name.
    }
    \item{\code{counter}: Node counter.
    }
    \item{\code{field_to_update}: Field to update value of.
    }
    \item{\code{update_value}: The value to update to.
    }
  }
}

\item{\code{submit_jobs}}{
 Submit jobs of the DAG given the node name(s) provided.
 If the node name is left to \code{NULL}, then all of the jobs are submitted.
  \emph{Arguments:}
  \itemize{
    \item{\code{nodename}: Name(s) of node to submit for job.
    }
  }
}

\item{\code{get_node_Rdata_path}}{
 Get the file path of the node with the environments saved out.
  \emph{Arguments:}
  \itemize{
    \item{\code{name}: Node name.
    }
    \item{\code{indic}: Indicator.
    }
    \item{\code{ig}: Indicator Group.
    }
    \item{\code{rd}: Run date.
    }
    \item{\code{age}: Age bin.
    }
    \item{\code{reg}: Region.
    }
    \item{\code{holdout}: Holdout fold.
    }
  }
}

\item{\code{remove_node}}{
 Drop node from the DAG. Note that this does not reset the node counter!
  \emph{Arguments:}
  \itemize{
    \item{\code{node_erase}: Node name(s) to erase.
    }
  }
}

\item{\code{wait_on_job}}{
 Generic method to check on job IDs on whether the job finished or not.
 Job tracking validation is done on whether the start and end job marker
 files are successfully written or not at different stages of the job.
 In order to tag a job as successfully finished, the job has to not exist
 in qstat, the start file must not exist, and the end file must exist.
  \emph{Arguments:}
  \itemize{
    \item{\code{job_ids}: Job IDs to track.
    }
    \item{\code{sleeptime}: Seconds to sleep until pinging qstat again.
    }
    \item{\code{tmpdir}: Temporary directory to track job markers.
    }
  }
}

\item{\code{wait_on_node}}{
 A wrapper on \code{wait_on_job} to take jobs in the DAG, and tag status.
 Multiple jobs are tracked parallelly using \code{foreach}.
  \emph{Arguments:}
  \itemize{
    \item{\code{nodename}: Node name(s) to track and tag status.
    }
    \item{\code{...}: All other arguments to pas to \code{wait_on_job}.
    }
  }
}

}
}

\examples{
\dontrun{
## Create a Pipeline object apriori

## Create a new MBGDag
DAG_object <- MBGDag$new(
  pipeline_class = Pipeline_Class,
  reg = test_reg,
  age = test_age,
  holdout = test_holdout, dag_hash = "vmpel"
)

## Create a node
DAG_object$create_node(
  nodename = "data_prep",
  jobscript = paste0(
    "/ihme/code/geospatial/sadatnfs/lbd_core/pipeline/scripts/01_PM_Data_Prepping.R"
  ),
  project = "proj_geo_nodes",
  cores = 1,
  ram_gb = 4,
  runtime = "00:10:00",
  queue = "geospatial.q",
  singularity_version = paste0(
    "/ihme/singularity-images/lbd/",
    "testing_INLA_builds/lbd_rpkgs3.6.0gcc9mkl",
    "rstudioserver1.2.1511_v4.simg"
  )
)


## Save DAG
print(
  paste0(
    "Saving DAG for row: ", test_row, " in LV table to:",
    DAG_object$get_prerun_image_path(file_format = ".rds")
  )
)
saveRDS(object = DAG_object, file = DAG_object$img_path)


### Preview DAG
DAG_object$DAG

## Submit all the jobs and track the jobs
DAG_object$submit_jobs()
DAG_object$wait_on_node()
saveRDS(object = DAG_object, file = DAG_object$img_path)
}

}
\keyword{MBG}
\keyword{parallel_model}
